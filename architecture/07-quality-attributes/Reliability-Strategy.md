# Reliability Strategy

## ğŸ“‹ PropÃ³sito

Define la estrategia para lograr **99.9% uptime** (SLO) y minimizar el impacto de fallos.

## ğŸ¯ Reliability Goals

| Metric                                | Target      | Acceptable Downtime               |
| ------------------------------------- | ----------- | --------------------------------- |
| **Availability (SLO)**                | 99.9%       | 43.8 min/mes                      |
| **MTBF** (Mean Time Between Failures) | > 720 horas | 1 fallo cada 30 dÃ­as              |
| **MTTR** (Mean Time To Recovery)      | < 15 min    | Recovery rÃ¡pido                   |
| **Error Budget**                      | 0.1%        | ~44 min/mes de downtime permitido |

---

## ğŸ›¡ï¸ Design for Failure

**Principio**: Asumir que componentes FALLARÃN. DiseÃ±ar para resilience.

### Failure Modes

| Componente                | Failure Mode             | Probabilidad | Impacto                       |
| ------------------------- | ------------------------ | ------------ | ----------------------------- |
| **Pod crash**             | OOMKilled, panic         | Alta         | Bajo (otro pod sirve traffic) |
| **Node failure**          | Hardware, kernel panic   | Media        | Medio (K8s reschedule pods)   |
| **DB failure**            | Primary down             | Baja         | Alto (writes fallan)          |
| **Network partition**     | Split brain              | Baja         | Alto (inconsistencia)         |
| **External service down** | Stripe, SendGrid offline | Media        | Medio (degraded mode)         |
| **Data center outage**    | Zona completa down       | Muy baja     | CrÃ­tico                       |

---

## ğŸ”„ Redundancy

### 1. Pod Redundancy

**MÃ­nimo 2 rÃ©plicas** por servicio (alta disponibilidad):

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  replicas: 3 # MÃ­nimo 3 para tolerar 1 failure
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1 # MÃ¡x 1 pod down durante update
      maxSurge: 1 # MÃ¡x 1 pod extra durante rollout
```

**Pod Anti-Affinity** (distribuir en diferentes nodos):

```yaml
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: order-service
        topologyKey: kubernetes.io/hostname
```

**Resultado**: Pods se despliegan en nodos diferentes. Si 1 nodo cae, otros siguen sirviendo.

---

### 2. Database Redundancy

#### PostgreSQL High Availability

**Setup**: Primary + Standby (hot standby)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Primary    â”‚
â”‚  (RW)       â”‚â”€â”€â”€â”€â”€â”€â”€â” Streaming
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ Replication
                      â”‚
                â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                â”‚  Standby   â”‚
                â”‚  (Readonly) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Failover**: Si primary cae, standby se promociona automÃ¡ticamente.

**Implementation** (con Patroni):

```yaml
# Patroni config
scope: postgres-cluster
name: postgres-1

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    postgresql:
      use_pg_rewind: true
      parameters:
        max_connections: 200
        shared_buffers: 2GB
```

**Failover time**: ~10-30 segundos (automÃ¡tico)

---

### 3. Message Queue Redundancy

**RabbitMQ Cluster** (3 nodos):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node 1  â”‚â”€â”¤  Node 2  â”‚â”€â”¤  Node 3  â”‚
â”‚ (Master) â”‚  â”‚ (Mirror) â”‚  â”‚ (Mirror) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Mirrored Queues**:

```bash
rabbitmqctl set_policy ha-all "^" '{"ha-mode":"all", "ha-sync-mode":"automatic"}'
```

**Resultado**: Si un nodo cae, otro toma ownership de las queues.

---

## âš¡ Circuit Breaker Pattern

**Problema**: Servicio dependiente estÃ¡ caÃ­do â†’ requests se acumulan â†’ cascading failure.

**SoluciÃ³n**: Circuit breaker detecta fallos y "abre el circuito" (falla rÃ¡pido).

### States

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  Failures    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLOSED  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   OPEN   â”‚
â”‚ (Normal) â”‚  > Threshold â”‚ (Failing)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚                          â”‚
     â”‚ Successful               â”‚ Timeout
     â”‚ Request                  â”‚ Elapsed
     â”‚                          â”‚
     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
     â””â”€â”€â”€â”€â”¤  HALF-OPEN   â”‚â—„â”€â”€â”€â”€â”€â”˜
          â”‚  (Testing)   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation (Node.js)

```typescript
import CircuitBreaker from 'opossum';

// Stripe payment call con circuit breaker
const options = {
  timeout: 5000, // 5s timeout
  errorThresholdPercentage: 50, // Abrir si > 50% errores
  resetTimeout: 30000, // 30s antes de intentar cerrar
};

const breaker = new CircuitBreaker(stripeClient.createPayment, options);

// Fallback cuando circuito estÃ¡ abierto
breaker.fallback(() => {
  return {
    status: 'PENDING',
    message: 'Payment system temporarily unavailable',
  };
});

// Events
breaker.on('open', () => {
  console.error('Circuit opened - Stripe is down');
  alerting.notify('Stripe circuit opened');
});

breaker.on('halfOpen', () => {
  console.log('Circuit half-open - testing Stripe');
});

// Usar
const result = await breaker.fire(paymentData);
```

---

## ğŸ” Retry Strategy

**Principio**: Fallos transitorios (network glitch, timeout) deben retryarse.

### Exponential Backoff

```typescript
import retry from 'async-retry';

await retry(
  async (bail) => {
    try {
      return await externalAPI.call();
    } catch (error) {
      if (error.status === 400) {
        // Client error - no retry
        bail(error);
      }
      // Transient error - retry
      throw error;
    }
  },
  {
    retries: 3,
    factor: 2, // 2^n backoff
    minTimeout: 1000, // 1s inicial
    maxTimeout: 10000,
    onRetry: (error, attempt) => {
      console.log(`Retry attempt ${attempt}: ${error.message}`);
    },
  }
);
```

**Backoff times**:

- Attempt 1: Wait 1s
- Attempt 2: Wait 2s
- Attempt 3: Wait 4s
- Attempt 4: Fail

**Idempotency**: Retries solo para operaciones idempotentes (GET, PUT, DELETE). POST requiere idempotency key.

---

## â±ï¸ Timeouts

**Problema**: Servicio lento bloquea threads â†’ agota pool â†’ cascading failure.

**SoluciÃ³n**: Timeouts agresivos en todos los external calls.

```typescript
// HTTP client con timeout
import axios from 'axios';

const httpClient = axios.create({
  timeout: 5000, // 5s timeout
  validateStatus: (status) => status < 500,
});

// Database query timeout
await prisma.$queryRaw`SELECT * FROM orders WHERE ...`.timeout(3000);

// Redis timeout
await redis.get('key', { timeout: 1000 });
```

**Valores recomendados**:

- External HTTP: 5s
- Database query: 3s
- Redis: 1s
- Internal service: 10s

---

## ğŸ”’ Bulkhead Pattern

**Principio**: Aislar recursos para prevenir que un failure afecte todo el sistema.

### Thread Pools Separados

```typescript
// Pool para external calls (limitado)
const externalPool = new ThreadPool({ maxWorkers: 5 });

// Pool para internal operations (mÃ¡s grande)
const internalPool = new ThreadPool({ maxWorkers: 20 });

// Si external API se atasca, solo consume 5 workers
await externalPool.exec(() => stripeClient.call());

// Internal operations siguen funcionando
await internalPool.exec(() => orderService.process());
```

### Database Connection Pools Separados

```typescript
// Pool para queries crÃ­ticos (garantizado)
const criticalPool = new Pool({ min: 5, max: 10 });

// Pool para queries no crÃ­ticos (best effort)
const nonCriticalPool = new Pool({ min: 2, max: 5 });

// Order creation usa critical pool
await criticalPool.query('INSERT INTO orders...');

// Analytics usa non-critical pool
await nonCriticalPool.query('SELECT COUNT(*) FROM orders...');
```

---

## ğŸ¥ Health Checks

### Liveness Probe

**Pregunta**: Â¿EstÃ¡ el pod vivo?

```typescript
// /health/liveness
app.get('/health/liveness', (req, res) => {
  // Simple check: proceso responde
  res.status(200).json({ status: 'ok' });
});
```

**K8s config**:

```yaml
livenessProbe:
  httpGet:
    path: /health/liveness
    port: 3000
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3 # 3 fallos â†’ restart pod
```

---

### Readiness Probe

**Pregunta**: Â¿EstÃ¡ el pod listo para recibir trÃ¡fico?

```typescript
// /health/readiness
app.get('/health/readiness', async (req, res) => {
  try {
    // Check DB connection
    await prisma.$queryRaw`SELECT 1`;

    // Check Redis
    await redis.ping();

    // Check RabbitMQ
    await rabbitMQ.checkConnection();

    res.status(200).json({ status: 'ready' });
  } catch (error) {
    res.status(503).json({ status: 'not ready', error: error.message });
  }
});
```

**K8s config**:

```yaml
readinessProbe:
  httpGet:
    path: /health/readiness
    port: 3000
  initialDelaySeconds: 10
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3 # 3 fallos â†’ remove from service
```

**Diferencia**:

- **Liveness fail** â†’ K8s **restart** pod
- **Readiness fail** â†’ K8s **remove** pod del load balancer (no mata)

---

## ğŸ”§ Graceful Shutdown

**Problema**: Al hacer deploy, K8s mata pods abruptamente â†’ requests in-flight fallan.

**SoluciÃ³n**: Graceful shutdown.

```typescript
// Capturar SIGTERM (K8s envÃ­a esto antes de matar)
process.on('SIGTERM', async () => {
  console.log('SIGTERM received, starting graceful shutdown');

  // 1. Stop accepting new requests
  server.close(() => {
    console.log('HTTP server closed');
  });

  // 2. Wait for in-flight requests to complete (max 30s)
  await new Promise((resolve) => {
    setTimeout(resolve, 30000);
  });

  // 3. Close DB connections
  await prisma.$disconnect();
  await redis.quit();

  // 4. Exit
  process.exit(0);
});
```

**K8s terminationGracePeriodSeconds**:

```yaml
spec:
  terminationGracePeriodSeconds: 30 # Wait 30s antes de SIGKILL
```

---

## ğŸš¨ Alerting

### Critical Alerts (PagerDuty)

```yaml
# Prometheus alerts
groups:
  - name: critical
    rules:
      - alert: ServiceDown
        expr: up{job="order-service"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: 'Order service is down'

      - alert: DatabaseDown
        expr: pg_up == 0
        for: 30s
        labels:
          severity: critical

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: 'Error rate > 5%'
```

---

## ğŸ“Š Monitoring

### SLI (Service Level Indicators)

```
Availability = Successful requests / Total requests Ã— 100%

Target: 99.9%
Error budget: 0.1% = 43.8 min/month
```

**Dashboard**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Availability (Last 30 days)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Current:     99.95%   âœ…               â”‚
â”‚  Target:      99.90%                    â”‚
â”‚  Error Budget Remaining: 21.9 min       â”‚
â”‚                                         â”‚
â”‚  Incidents:                             â”‚
â”‚  - 2025-12-15: 5 min (DB failover)      â”‚
â”‚  - 2025-12-01: 10 min (deploy issue)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª Chaos Engineering (Fase 3)

**Objetivo**: Proactivamente inyectar fallos para validar resilience.

### Chaos Monkey (Netflix)

```bash
# Aleatoriamente mata pods en production
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: chaos-monkey
spec:
  containers:
  - name: chaos-monkey
    image: chaostoolkit/chaostoolkit
    command: ["chaos", "run", "kill-random-pod.yaml"]
EOF
```

### Tests

- **Kill random pod** â†’ Sistema sigue funcionando?
- **Introduce network latency** â†’ Timeouts funcionan?
- **Fail database primary** â†’ Failover automÃ¡tico?
- **Spike traffic 10x** â†’ Auto-scaling funciona?

---

## âœ… Reliability Checklist

### Application

- [ ] MÃ­nimo 2 rÃ©plicas por servicio
- [ ] Health checks (liveness + readiness) implementados
- [ ] Graceful shutdown implementado
- [ ] Circuit breakers en external calls
- [ ] Retry con exponential backoff
- [ ] Timeouts en todos los external calls

### Infrastructure

- [ ] Database HA (primary + standby)
- [ ] RabbitMQ cluster (3 nodos)
- [ ] Redis Sentinel (3 nodos)
- [ ] Pod anti-affinity configurado
- [ ] PodDisruptionBudget configurado

### Monitoring

- [ ] Availability metrics trackeadas
- [ ] Error budget calculado
- [ ] Alertas crÃ­ticas configuradas (PagerDuty)
- [ ] Runbooks para incidents documentados
- [ ] Post-mortem process definido

---

**VersiÃ³n**: 1.0  
**Ãšltima actualizaciÃ³n**: Diciembre 2025
